{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                       PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pedir Archivos De Github\n",
    "\n",
    "Codigo automatizado con Cloud Scheduler para que se active cada cierto tiempo. Agarra los Datasets de Github en crudo y los tira en Cloud Store.\n",
    "\n",
    "El token para autenticar el github fue sacado y reemplazado por \"x\" por seguridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.cloud import storage\n",
    "\n",
    "def descargar_y_cargar_archivos_gcs(request):\n",
    "    # Token de autenticación para la API de GitHub\n",
    "    github_token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "    # URL del repositorio y ruta donde se encuentran los archivos\n",
    "    repo_url = \"https://api.github.com/repos/darksider10/PF/contents/Datasets\"\n",
    "\n",
    "    # Encabezados para la solicitud con el token de autenticación\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {github_token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "\n",
    "    # Realizar solicitud GET a la API de GitHub\n",
    "    response = requests.get(repo_url, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Crear una instancia de cliente de Google Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Obtener un objeto de cubo (bucket) de almacenamiento\n",
    "    bucket = client.get_bucket('dastorage')\n",
    "\n",
    "    # Descargar archivos y cargarlos a Google Cloud Storage\n",
    "    for file_data in data:\n",
    "        if file_data[\"type\"] == \"file\" and (file_data[\"name\"].endswith('.dbf') or file_data[\"name\"].endswith('.csv')):\n",
    "            file_url = file_data[\"download_url\"]\n",
    "            file_name = file_data[\"name\"]\n",
    "            file_content = requests.get(file_url, headers=headers).content\n",
    "\n",
    "#Crear un objeto Blob en el cubo de almacenamiento\n",
    "            blob = bucket.blob(file_name)\n",
    "\n",
    "#Cargar el contenido del archivo al objeto Blob\n",
    "            blob.upload_from_string(file_content)\n",
    "\n",
    "    return \"Archivos cargados exitosamente en Google Cloud Storage.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scrapping con archivos de Taxis amarillos del año 2022, se agarran los 12 meses del año por separados y se carga en Cloud Storage con esta Cloud Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from google.cloud import storage\n",
    "\n",
    "def download_and_upload_parquet_files(request):\n",
    "    # URL del sitio web a realizar el web scraping\n",
    "    website = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    response = requests.get(website)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Encontrar enlaces que apunten a archivos .parquet\n",
    "    links = soup.find_all(\"a\", href=lambda href: href and \"yellow_tripdata_2022\" in href and \".parquet\")\n",
    "\n",
    "    # Inicializar el cliente de Google Cloud Storage\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket('parquetsws')  # Reemplaza 'tu_bucket' con el nombre de tu bucket\n",
    "\n",
    "    for link in links:\n",
    "        # Descargar el archivo .parquet\n",
    "        file_url = link[\"href\"]\n",
    "        file_name = file_url.split(\"/\")[-1]  # Obtener el nombre del archivo de la URL\n",
    "        response = requests.get(file_url)\n",
    "        if response.status_code == 200:\n",
    "            # Subir el archivo .parquet a Google Cloud Storage\n",
    "            blob = bucket.blob(file_name)\n",
    "            blob.upload_from_string(response.content)\n",
    "            print(f\"Archivo {file_name} subido a Google Cloud Storage.\")\n",
    "        else:\n",
    "            print(f\"No se pudo descargar el archivo {file_url}.\")\n",
    "\n",
    "    return \"Proceso completado.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Carga Parquets a BigQuery\n",
    "\n",
    "Unifica todos los parquets en uno solo y modifica el codigo, como son demasiados registros solo dejo 10.000 por cada mes y lo guardo.\n",
    "\n",
    "Esta Cloud Function es un Trigger y se activa cuando entran archivos .parquet al Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage, bigquery\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataset(project_id, dataset_id):\n",
    "    bigquery_client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    \n",
    "    try:\n",
    "        bigquery_client.create_dataset(dataset)\n",
    "        print(f\"Dataset {dataset_id} creado exitosamente en BigQuery.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al crear el dataset: {e}\")\n",
    "\n",
    "def process_file(data, context):\n",
    "    # Obtiene la información del archivo creado\n",
    "    bucket_name = data['bucket']\n",
    "    file_name = data['name']\n",
    "\n",
    "    # Configura el cliente de Google Cloud Storage y BigQuery\n",
    "    storage_client = storage.Client()\n",
    "    bigquery_client = bigquery.Client()\n",
    "\n",
    "    # Lee el contenido del archivo desde Cloud Storage\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    file_content = blob.download_as_string()\n",
    "\n",
    "    # Utiliza io.BytesIO para crear un objeto de archivo en memoria\n",
    "    file_content_as_file = io.BytesIO(file_content)\n",
    "\n",
    "    # Define el nombre de la tabla en BigQuery\n",
    "    table_id = 'yellow_tripdata_2022'\n",
    "\n",
    "    if file_name.endswith('.parquet'):\n",
    "        # Lee el archivo Parquet usando pyarrow\n",
    "        table = pq.read_table(file_content_as_file)\n",
    "\n",
    "        # Convierte la tabla Parquet a un DataFrame de pandas\n",
    "        df_parquet = table.to_pandas()\n",
    "\n",
    "        df_parquet[\"PickupZone\"] = df_parquet[\"PULocationID\"].apply(get_borough)\n",
    "\n",
    "        df_parquet = df_parquet.dropna()\n",
    "\n",
    "        # Selecciona solo las columnas requeridas\n",
    "        df_parquet = df_parquet.sample(n=10000, random_state=42)[['tpep_pickup_datetime', 'tpep_dropoff_datetime', \n",
    "                                        'passenger_count', 'trip_distance', \n",
    "                                        'PULocationID', 'DOLocationID', \n",
    "                                        'total_amount','PickupZone']]\n",
    "        \n",
    "\n",
    "        # Crea un archivo temporal CSV para cargar en BigQuery\n",
    "        csv_content_as_file = io.StringIO()\n",
    "        df_parquet.to_csv(csv_content_as_file, index=False)\n",
    "\n",
    "        # Mueve el cursor al inicio del archivo CSV\n",
    "        csv_content_as_file.seek(0)\n",
    "\n",
    "        # Configura el job de carga en BigQuery\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            autodetect=True,\n",
    "        )\n",
    "\n",
    "        # Carga el contenido del archivo CSV en BigQuery\n",
    "        table_ref = bigquery_client.dataset('ETL').table(table_id)\n",
    "        load_job = bigquery_client.load_table_from_file(csv_content_as_file, table_ref, job_config=job_config)\n",
    "        load_job.result()  # Espera a que termine la carga\n",
    "\n",
    "        print(f'Todos los archivos Parquet han sido cargados en BigQuery con el nombre de tabla {table_id}.')\n",
    "\n",
    "def get_borough(location_id):\n",
    "            # Agrupamos los PULocationID y DOLocationID de acuerdo a su Borough correspondiente\n",
    "    zones = {   #Manhattan\n",
    "                \"Manhattan\" : [4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 103, 103, 103, 107, 113, 114, 116, 120, 125,\n",
    "                            127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211,\n",
    "                            224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246, 249, 261, 262, 263],\n",
    "\n",
    "                #Brooklyn\n",
    "                \"Brooklyn\" : [11, 14, 17, 21, 22, 25, 26, 29, 33, 34, 35, 36, 37, 39, 40, 49, 52, 54, 55, 61, 62, 63, 65, 66, 67, 71, 72, 76, 77, 80,\n",
    "                            85, 89, 91, 97, 106, 108, 111, 112, 123, 133, 149, 150, 154, 155, 165, 177, 178, 181, 188, 189, 190, 195, 210, 217, 222,\n",
    "                            225, 227, 228, 255, 256, 257],\n",
    "\n",
    "                #Queens\n",
    "                \"Queens\" : [2, 7, 8, 9, 10, 15, 16, 19, 27, 28, 30, 38, 53, 56, 56, 64, 70, 73, 82, 83, 86, 92, 93, 95, 96, 98, 101, 102, 117, 121,\n",
    "                        122, 124, 129, 130, 131, 132, 134, 135, 138, 139, 145, 146, 157, 160, 171, 173, 175, 179, 180, 191, 192, 193, 196, 197,\n",
    "                        198, 201, 203, 205, 207, 215, 216, 218, 219, 223, 226, 252, 253, 258, 260],\n",
    "\n",
    "                #Bronx\n",
    "                \"Bronx\" : [3, 18, 20, 31, 32, 46, 47, 51, 58, 59, 60, 69, 78, 81, 94, 119, 126, 136, 147, 159, 167, 168, 169, 174, 182, 183, 184,\n",
    "                        185, 199, 200, 208, 212, 213, 220, 235, 240, 241, 242, 247, 248, 250, 254, 259],\n",
    "                #State Island\n",
    "                \"State Island\" : [5, 6, 23, 44, 84, 99, 109, 110, 115, 118, 156, 172, 176, 187, 204, 206, 214, 221, 245, 251],\n",
    "\n",
    "                #EWR\n",
    "               \"EWR\" : [1]}\n",
    "\n",
    "    for borough, ids in zones.items():\n",
    "        if location_id in ids:\n",
    "            return borough\n",
    "    return \"Unknow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   ETL de Cloud Storage a BigQuery de Archivos .csv Crudos\n",
    "\n",
    "Esta funcion es un Trigger que se activa cuando entran los archivos .csv al Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.exceptions import NotFound\n",
    "from google.cloud import storage, bigquery\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_file(data, context):\n",
    "    # Obtiene la información del archivo creado\n",
    "    bucket_name = data['bucket']\n",
    "    file_name = data['name']\n",
    "\n",
    "    # Configura el cliente de Google Cloud Storage y BigQuery\n",
    "    storage_client = storage.Client()\n",
    "    bigquery_client = bigquery.Client()\n",
    "\n",
    "    # Lee el contenido del archivo desde Cloud Storage\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    file_content = blob.download_as_text()\n",
    "    \n",
    "    # Si el archivo es 'Alternative Fuel Vehicles US.csv', aplicamos transformaciones\n",
    "    if 'Alternative_Fuel_Vehicles_US.csv' in file_name:\n",
    "        # Realiza transformaciones en el archivo y guarda el resultado en file_content_as_file\n",
    "        \n",
    "        # Transformar el contenido del archivo en un DataFrame de pandas\n",
    "        df = pd.read_csv(io.StringIO(file_content))\n",
    "\n",
    "        #-------------------------------------------------------------------------------------\n",
    "        # Eliminar columnas innecesarias del DataFrame\n",
    "        df.drop(columns=['Heavy-Duty Power System', 'Notes', 'Drivetrain', 'Number of Passengers',\n",
    "        'PHEV Total Range', 'Engine Cylinder Count', 'Transmission Make', 'Transmission Type', \n",
    "        'All-Electric Range', 'Alternative Fuel Economy City', 'Alternative Fuel Economy Highway', 'Alternative Fuel Economy Combined',\n",
    "        'Conventional Fuel Economy City', 'Conventional Fuel Economy Highway', 'Conventional Fuel Economy Combined'], axis=1, inplace=True)\n",
    "\n",
    "        # Definir las categorías a eliminar\n",
    "        categories_to_remove = [\n",
    "            'Refuse', 'School Bus', 'Street Sweeper', 'Tractor',\n",
    "            'TractorVocational/Cab Chassis', 'Transit Bus',\n",
    "            'Vocational/Cab Chassis', 'Vocational/Cab ChassisTractor',\n",
    "            'Passenger Van/Shuttle Bus', 'Van', 'Pickup', 'Step Van', 'Vocational/Cab ChassisVan'\n",
    "        ]\n",
    "\n",
    "        # Filtrar el DataFrame para eliminar las categorías especificadas\n",
    "        df = df[~df['Category'].isin(categories_to_remove)]\n",
    "\n",
    "        # Normalizar los valores de la columna 'Engine Size' para que todos tengan el formato 'X kW'\n",
    "        df['Engine Size'] = df['Engine Size'].str.replace(r'(\\d+)\\s*[kK][wW].*', r'\\1 kW', regex=True)\n",
    "\n",
    "        # Reemplazar abreviaturas en la columna 'Engine Type' por su nombre completo\n",
    "        df['Engine Type'] = df['Engine Type'].replace({\n",
    "            'SI': 'Spark Ignition',\n",
    "            'e-motor': 'Electric Motor',\n",
    "            'CI': 'Compression Ignition',\n",
    "            'FC': 'Fuel Cell'\n",
    "        })\n",
    "\n",
    "        # Eliminar filas con valores faltantes en las columnas 'Engine Type' y 'Engine Size'\n",
    "        df = df.dropna(subset=['Engine Type', 'Engine Size'])\n",
    "\n",
    "        # Eliminar filas duplicadas\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        # Renombrar las columnas del DataFrame\n",
    "        df.columns = [\n",
    "            'Categoría', 'Modelo', 'AñoModelo', 'Fabricante', \n",
    "            'Combustible', 'TipoMotor', 'TamañoMotor'\n",
    "        ]\n",
    "\n",
    "        df[\"ID\"] = df.index\n",
    "\n",
    "        # Convertir el DataFrame de pandas transformado de nuevo a un archivo CSV en memoria\n",
    "        transformed_file = io.StringIO()\n",
    "        df.to_csv(transformed_file, index=False)\n",
    "        transformed_file.seek(0)\n",
    "        \n",
    "        # Usaremos 'transformed_file' en lugar de 'file_content_as_file' para la carga en BigQuery\n",
    "        file_content_as_file = transformed_file\n",
    "        # Define el nombre de la tabla en BigQuery usando el nombre del archivo sin extensión como tabla\n",
    "        table_id = file_name.split('.')[0]\n",
    "    \n",
    "    elif 'Electric_and_Alternative_Fuel_Charging_Stations.csv' in file_name:\n",
    "       \n",
    "        df_ny = pd.read_csv(io.StringIO(file_content))\n",
    "\n",
    "        #-------------------------------------------------------------------------------\n",
    "        # Filtrar las filas donde la columna 'State' es igual a 'NY'\n",
    "        df_ny = df_ny[df_ny['State'] == 'NY']\n",
    "\n",
    "        # Lista de columnas a eliminar\n",
    "        columns_to_drop = [\n",
    "            \"Intersection Directions\", \"Plus4\", \"Expected Date\", \"Cards Accepted\",\n",
    "            \"BD Blends\", \"NG Fill Type Code\", \"NG PSI\", \"EV Other Info\",\n",
    "            \"Federal Agency ID\", \"Federal Agency Name\", \"Federal Agency Code\",\n",
    "            \"Geocode Status\", \"Date Last Confirmed\", \"Owner Type Code\", \"Facility Type\",\n",
    "            \"CNG Dispenser Num\", \"CNG On-Site Renewable Source\",\n",
    "            \"CNG Total Compression Capacity\", \"CNG Storage Capacity\",\n",
    "            \"LNG On-Site Renewable Source\", \"E85 Other Ethanol Blends\",\n",
    "            \"LPG Nozzle Types\", \"Hydrogen Pressures\", \"Hydrogen Standards\",\n",
    "            \"CNG Fill Type Code\", \"CNG PSI\", \"CNG Vehicle Class\",\n",
    "            \"LNG Vehicle Class\", \"EV On-Site Renewable Source\",\n",
    "            \"Intersection Directions (French)\", \"Access Days Time (French)\",\n",
    "            \"BD Blends (French)\", \"Groups With Access Code (French)\", \"Hydrogen Status Link\", \"LPG Primary\",\n",
    "            \"E85 Blender Pump\", \"Hydrogen Is Retail\", \"EV Connector Types\", \"ZIP\", \"EV Level2 EVSE Num\", \"Open Date\",\"EV Pricing\",\n",
    "            \"EV DC Fast Count\", \"Restricted Access\", \"Access Days Time\", \"Access Detail Code\", \"Updated At\", \"Status Code\", \"Access Code\"\n",
    "        ]\n",
    "\n",
    "        # Eliminar las columnas del dataframe\n",
    "        df_ny.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "        # Aplicar una función para simplificar los valores de \"Groups With Access Code\" a \"Public\" o \"Private\"\n",
    "        df_ny['Groups With Access Code'] = df_ny['Groups With Access Code'].apply(lambda x: \"Public\" if \"Public\" in x else (\"Private\" if \"Private\" in x else x))\n",
    "\n",
    "        # Eliminar columnas  del DataFrame\n",
    "        columns_to_drop = ['Station Phone', 'EV Network', 'EV Network Web', 'NG Vehicle Class', 'Country', 'EV Pricing (French)', 'EV Level1 EVSE Num' ]\n",
    "        df_ny.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "        # Traducción y Renombramiento de columnas al español\n",
    "        columnas_en_espanol = {\n",
    "            'Fuel Type Code': 'CodigoCombustible',\n",
    "            'Station Name': 'NombreEstacion',\n",
    "            'Street Address': 'Dirección',\n",
    "            'City': 'Ciudad',\n",
    "            'State': 'Estado',\n",
    "            'Groups With Access Code': 'AccesoPublico',\n",
    "            'Latitude': 'Latitud',\n",
    "            'Longitude': 'Longitud',\n",
    "            'ID': 'IDEstacion',\n",
    "        }\n",
    "        df_ny.rename(columns=columnas_en_espanol, inplace=True)\n",
    "\n",
    "        # ajuste de datos\n",
    "        df_ny['AccesoPublico'] = df_ny['AccesoPublico'].replace({'Public': True, 'Private': False})\n",
    "\n",
    "        df_ny[\"ID\"] = df_ny.index\n",
    "\n",
    "        transformed_file = io.StringIO()\n",
    "        df_ny.to_csv(transformed_file, index=False)\n",
    "        transformed_file.seek(0)\n",
    "        \n",
    "        file_content_as_file = transformed_file\n",
    "        table_id = file_name.split('.')[0]\n",
    "\n",
    "    elif 'Light_Duty_Vehicles.csv' in file_name:\n",
    "\n",
    "        df = pd.read_csv(io.StringIO(file_content))\n",
    "\n",
    "        #------------------------------------------------------------------------------\n",
    "        # Eliminar columnas que no vamos a utilizar porque no necesitamos la información o porque la misma se ennuentra disponible de forma más detallada en otro dataset.\n",
    "\n",
    "        df.drop(columns=[ 'Notes', 'Drivetrain', 'PHEV Total Range', 'Engine Cylinder Count','Transmission Type', 'Engine Description','Fuel Configuration ID', \n",
    "        'Manufacturer URL', 'Fuel Code', 'PHEV Type', 'Alternative Fuel Economy City', 'Alternative Fuel Economy Highway','Alternative Fuel Economy Combined', \n",
    "        'Conventional Fuel Economy City', 'Conventional Fuel Economy Highway', 'Conventional Fuel Economy Combined', 'Engine Type', 'Engine Size', \n",
    "        'Fuel Configuration Name', 'Electric-Only Range'], axis=1, inplace=True)\n",
    "\n",
    "        # Filtramos el dataset para incluir solo \"Sedans\" y \"SUVs\"\n",
    "        df = df[df['Category'].isin(['Sedan', 'SUV'])]\n",
    "\n",
    "        # Filtrar el DataFrame para incluir solo registros desde 2017 hasta 2022\n",
    "        df = df[(df['Model Year'] >= 2017) & (df['Model Year'] <= 2022)]\n",
    "\n",
    "        # Cambiar los nombres de las columnas al español\n",
    "        df.columns = [\n",
    "            'IDVehículo', 'IDCombustible', 'IDFabricante', \n",
    "            'IDCategoría', 'Modelo', 'AñoModelo', \n",
    "            'Fabricante', 'Categoría', 'Combustible'\n",
    "        ]\n",
    "\n",
    "        \n",
    "        transformed_file = io.StringIO()\n",
    "        df.to_csv(transformed_file, index=False)\n",
    "        transformed_file.seek(0)\n",
    "        \n",
    "        file_content_as_file = transformed_file\n",
    "        table_id = file_name.split('.')[0]\n",
    "\n",
    "    elif 'Vehicle_Fuel_Economy_Data.csv' in file_name:\n",
    "\n",
    "        df = pd.read_csv(io.StringIO(file_content))\n",
    "\n",
    "        #------------------------------------------------------------------------------\n",
    "        # Las columnas que vamos a dejar en el dataset por su importancia para nuestro objetivo\n",
    "        columns_to_keep_corrected = [\n",
    "            'Year', 'Manufacturer', 'Model', 'VClass', 'fuelType', 'fuelType1', \n",
    "            'city08', 'highway08', 'comb08', \n",
    "            'cityA08', 'highwayA08', 'combA08', \n",
    "            'co2', 'co2TailpipeGpm', \n",
    "            'phevBlended', \n",
    "            'range',\n",
    "            'fuelCost08', \n",
    "            'fuelCostA08', \n",
    "        ]\n",
    "        # Filtramos el dataframe para mantener solo las columnas seleccionadas\n",
    "        df = df[columns_to_keep_corrected]\n",
    "\n",
    "\n",
    "        # Eliminamos duplicados estrictos (donde todas las columnas relevantes son idénticas)\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "        # Filtrar el dataframe para incluir solo vehículos desde el año 2017 en adelante\n",
    "        df = df[df['Year'] >= 2017]\n",
    "\n",
    "        #  Cambiar los nombres de las columnas al español\n",
    "        df.rename(columns={\n",
    "            'Year': 'Año',\n",
    "            'Manufacturer': 'Fabricante',\n",
    "            'Model': 'Modelo',\n",
    "            'VClass': 'ClaseVehículo',\n",
    "            'fuelType': 'TipoCombustible',\n",
    "            'fuelType1': 'TipoCombustible1',\n",
    "            'city08': 'Ciudad08',\n",
    "            'highway08': 'Carretera08',\n",
    "            'comb08': 'Combinado08',\n",
    "            'cityA08': 'CiudadA08',\n",
    "            'highwayA08': 'CarreteraA08',\n",
    "            'combA08': 'CombinadoA08',\n",
    "            'co2': 'CO2',\n",
    "            'co2TailpipeGpm': 'CO2TuboEscapeGpm',\n",
    "            'phevBlended': 'HibridoEnchufable',\n",
    "            'range': 'Rango',\n",
    "            'fuelCost08': 'CostoCombustible08',\n",
    "            'fuelCostA08': 'CostoCombustibleA08'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Eliminar 'TipoCombustible' y renombrar 'TipoCombustible1' a 'TipoCombustible'\n",
    "        df.drop(columns=['TipoCombustible'], inplace=True)\n",
    "        df.rename(columns={'TipoCombustible1': 'TipoCombustible'}, inplace=True)\n",
    "\n",
    "        # Eliminar 'CO2TuboEscapeGpm' y renombrar 'CO2' a 'CO2(g/pm)'\n",
    "        df.drop(columns=['CO2TuboEscapeGpm'], inplace=True)\n",
    "        df.rename(columns={'CO2': 'CO2(g/pm)'}, inplace=True)\n",
    "\n",
    "        # Renombrar 'Rango' a 'RangoAutonomia'\n",
    "        df.rename(columns={'Rango': 'RangoAutonomia'}, inplace=True)\n",
    "\n",
    "        # Eliminar 'CostoCombustibleA08' y renombrar 'CostoCombustible08' a 'CostoCombustible'\n",
    "        df.drop(columns=['CostoCombustibleA08'], inplace=True)\n",
    "        df.rename(columns={'CostoCombustible08': 'CostoCombustible'}, inplace=True)\n",
    "\n",
    "        # Eliminar columnas específicas de economía de combustible en ciudad y carretera\n",
    "        df.drop(columns=['Ciudad08', 'Carretera08', 'CiudadA08', 'CarreteraA08'], inplace=True)\n",
    "\n",
    "        # Renombrar columnas de eficiencia de combustible\n",
    "        df.rename(columns={'Combinado08': 'EficienciaConv', 'CombinadoA08': 'EficienciaAlt'}, inplace=True)\n",
    "        df = df.dropna()\n",
    "\n",
    "        df[\"ID\"] = df.index\n",
    "\n",
    "        df[\"Eficiencia\"] = np.where(df[\"EficienciaAlt\"] != 0, ((300 * df[\"EficienciaConv\"]) / (300 * df[\"EficienciaAlt\"])) * 100, np.nan)\n",
    "\n",
    "        \"\"\"Eficiencia=(Km recorridos por vehıˊculo convencional×Consumo de combustible total de vehıˊculos \n",
    "        convencionalesKm recorridos por vehıˊculo eleˊctrico×Consumo energeˊtico total de vehıˊculos eleˊctricos​)×100\"\"\"\n",
    "\n",
    "        transformed_file = io.StringIO()\n",
    "        df.to_csv(transformed_file, index=False)\n",
    "        transformed_file.seek(0)\n",
    "        \n",
    "        file_content_as_file = transformed_file\n",
    "        table_id = file_name.split('.')[0]\n",
    "        \n",
    "    elif 'Batteries.csv' in file_name:\n",
    "\n",
    "        df = pd.read_csv(io.StringIO(file_content))\n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        df[\"ID\"] = df.index\n",
    "        \n",
    "        transformed_file = io.StringIO()\n",
    "        df.to_csv(transformed_file, index=False)\n",
    "        transformed_file.seek(0)\n",
    "        \n",
    "        file_content_as_file = transformed_file\n",
    "        table_id = file_name.split('.')[0]\n",
    "\n",
    "    elif 'ElectricCarData_Clean.csv' in file_name:\n",
    "\n",
    "        electric_cardata = pd.read_csv(io.StringIO(file_content))\n",
    "        \n",
    "        #-----------------------------------------------------------------------------\n",
    "        electric_cardata[\"ID\"] = electric_cardata.index\n",
    "        \n",
    "        transformed_file = io.StringIO()\n",
    "        electric_cardata.to_csv(transformed_file, index=False)\n",
    "        transformed_file.seek(0)\n",
    "        \n",
    "        file_content_as_file = transformed_file\n",
    "        table_id = file_name.split('.')[0]\n",
    "        \n",
    "\n",
    "    else:\n",
    "        # Si el archivo no es el esperado, pasa el contenido original a BigQuery\n",
    "        file_content_as_file = io.StringIO(file_content)\n",
    "        table_id = file_name.split('.')[0]\n",
    "    \n",
    "    # Configura el job de carga en BigQuery\n",
    "    dataset_id = 'ETL'\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        autodetect=True,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    )\n",
    "\n",
    "    # Carga el contenido del archivo en BigQuery\n",
    "    dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    try:\n",
    "        # Verifica si la tabla ya existe\n",
    "        table = bigquery_client.get_table(table_ref)\n",
    "        print(f'Tabla {table_id} ya existe en BigQuery. Eliminando...')\n",
    "        # Si la tabla ya existe, elimina completamente la tabla\n",
    "        bigquery_client.delete_table(table_ref)\n",
    "        print(f'Tabla {table_id} eliminada de BigQuery.')\n",
    "    except NotFound:\n",
    "        # Si la tabla no existe, imprime un mensaje\n",
    "        print(f'Tabla {table_id} no encontrada en BigQuery. Creando...')\n",
    "\n",
    "    # Crea la tabla en BigQuery\n",
    "    table = bigquery.Table(table_ref)\n",
    "    table = bigquery_client.create_table(table)\n",
    "\n",
    "    # Carga el contenido del archivo en BigQuery\n",
    "    load_job = bigquery_client.load_table_from_file(file_content_as_file, table_ref, job_config=job_config)\n",
    "    load_job.result()  # Espera a que termine la carga\n",
    "\n",
    "    print(f'Archivo {file_name} cargado en BigQuery con el nombre de tabla {table_id}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Solucion De Problemas con los Parquets\n",
    "\n",
    "Ya que la funcion para pasar los parquets los va sumando esta funcion borra la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "def eliminar_tabla(event, context):\n",
    "    # Obtener el nombre del archivo cargado\n",
    "    file_name = event['name']\n",
    "\n",
    "    # Verificar si el nombre del archivo coincide con el patrón esperado\n",
    "    if 'yellow_tripdata_2022-01.parquet' in file_name:\n",
    "        # Eliminar la tabla en BigQuery\n",
    "        proyecto = \"swift-climate-415514\"\n",
    "        dataset = \"ETL\"\n",
    "        tabla_a_eliminar = f\"{proyecto}.{dataset}.yellow_tripdata_2022\"\n",
    "        eliminar_tabla_bigquery(tabla_a_eliminar)\n",
    "        print(f\"La tabla {tabla_a_eliminar} ha sido eliminada.\")\n",
    "\n",
    "def eliminar_tabla_bigquery(tabla):\n",
    "    # Inicializar el cliente de BigQuery\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Eliminar la tabla\n",
    "    bq_client.delete_table(tabla, not_found_ok=True)\n",
    "\n",
    "    # Imprimir mensaje de confirmación\n",
    "    print(f\"La tabla {tabla} ha sido eliminada.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   MACHINE LEARNING\n",
    "\n",
    "El modelo fue entrenado en local y el Deploy se hizo con Streamlit conectando con Github donde se encontraba el .PKL que es el modelo ya entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Entrenamiento del modelo\n",
    "\n",
    "Este modelo se entrena con ElasticNet que es un método de regularización utilizado en el aprendizaje automático y la estadística, especialmente en problemas de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "def get_borough(location_id):\n",
    "            # Agrupamos los PULocationID y DOLocationID de acuerdo a su Borough correspondiente\n",
    "    zones = {   #Manhattan\n",
    "                1 : [4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 103, 103, 103, 107, 113, 114, 116, 120, 125,\n",
    "                            127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211,\n",
    "                            224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246, 249, 261, 262, 263],\n",
    "\n",
    "                #Brooklyn\n",
    "                2 : [11, 14, 17, 21, 22, 25, 26, 29, 33, 34, 35, 36, 37, 39, 40, 49, 52, 54, 55, 61, 62, 63, 65, 66, 67, 71, 72, 76, 77, 80,\n",
    "                            85, 89, 91, 97, 106, 108, 111, 112, 123, 133, 149, 150, 154, 155, 165, 177, 178, 181, 188, 189, 190, 195, 210, 217, 222,\n",
    "                            225, 227, 228, 255, 256, 257],\n",
    "\n",
    "                #Queens\n",
    "                3 : [2, 7, 8, 9, 10, 15, 16, 19, 27, 28, 30, 38, 53, 56, 56, 64, 70, 73, 82, 83, 86, 92, 93, 95, 96, 98, 101, 102, 117, 121,\n",
    "                        122, 124, 129, 130, 131, 132, 134, 135, 138, 139, 145, 146, 157, 160, 171, 173, 175, 179, 180, 191, 192, 193, 196, 197,\n",
    "                        198, 201, 203, 205, 207, 215, 216, 218, 219, 223, 226, 252, 253, 258, 260],\n",
    "\n",
    "                #Bronx\n",
    "                4 : [3, 18, 20, 31, 32, 46, 47, 51, 58, 59, 60, 69, 78, 81, 94, 119, 126, 136, 147, 159, 167, 168, 169, 174, 182, 183, 184,\n",
    "                        185, 199, 200, 208, 212, 213, 220, 235, 240, 241, 242, 247, 248, 250, 254, 259],\n",
    "                #State Island\n",
    "                5 : [5, 6, 23, 44, 84, 99, 109, 110, 115, 118, 156, 172, 176, 187, 204, 206, 214, 221, 245, 251],\n",
    "\n",
    "                #EWR\n",
    "               6 : [1]}\n",
    "\n",
    "    for borough, ids in zones.items():\n",
    "        if location_id in ids:\n",
    "            return borough\n",
    "    return 7\n",
    "\n",
    "meses = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "df_meses = []\n",
    "\n",
    "for mes in meses:\n",
    "    df_mes = pd.read_parquet(f'yellow_tripdata_2022-{mes}.parquet')\n",
    "    df_mes = df_mes.sample(n=500, random_state=42).dropna()\n",
    "    df_mes[\"PickupZone\"] = df_mes[\"PULocationID\"].apply(get_borough)\n",
    "    df_mes['tpep_pickup_datetime'] = df_mes['tpep_pickup_datetime'].dt.hour\n",
    "    df_meses.append(df_mes)\n",
    "\n",
    "df = pd.concat(df_meses)\n",
    "X = df[[\"tpep_pickup_datetime\", 'trip_distance', 'PickupZone']]\n",
    "y = df['total_amount']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Utiliza ElasticNet en lugar de RandomForestRegressor\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Guarda el modelo entrenado para su uso en Streamlit\n",
    "joblib.dump(model, 'ModeloEntrenado.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este codigo se llama Stream.py y es el que estructura el Streamlit, se puede ver que se pide la Hora, las millas y la zona de donde se toma.\n",
    "\n",
    "Omito los requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import sklearn\n",
    "\n",
    "\n",
    "# Función para cargar el modelo desde GitHub\n",
    "def load_model():\n",
    "    # URL raw del archivo .pkl en GitHub\n",
    "    model_url = 'https://github.com/darksider10/PF/raw/main/ML/ModeloEntrenado.pkl'\n",
    "    \n",
    "    # Descargar el archivo .pkl desde la URL\n",
    "    response = requests.get(model_url)\n",
    "    \n",
    "    # Guardar el archivo .pkl localmente\n",
    "    with open('ModeloEntrenado.pkl', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Cargar el modelo desde el archivo descargado\n",
    "    model = joblib.load('ModeloEntrenado.pkl')\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "# Creando la interfaz de usuario\n",
    "st.title('Predicción del Total Amount para viajes de Taxi')\n",
    "\n",
    "tpep_pickup_datetime= st.number_input('Hora', min_value=0, max_value=23)\n",
    "trip_distance = st.number_input('Distancia del viaje (millas)', min_value=0.0, format=\"%.2f\")\n",
    "pickup_zone = st.selectbox('Zona de recogida', options=[('6','EWR'),('5', 'Staten Island'),('3', 'Queens'),('4', 'Bronx'), ('-1', 'Desconocido'),('2', 'Brooklyn'),('1', 'Manhattan')], format_func=lambda x: x[1])\n",
    "\n",
    "# Botón para realizar la predicción\n",
    "if st.button('Predecir Total Amount'):\n",
    "    X_new = pd.DataFrame([[tpep_pickup_datetime, trip_distance, int(pickup_zone[0])]], columns=['tpep_pickup_datetime', 'trip_distance', 'PickupZone'])\n",
    "    y_pred_new = model.predict(X_new)\n",
    "    st.success(f'La predicción del total_amount es: ${y_pred_new[0]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
